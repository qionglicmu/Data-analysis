{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/qiongli/Dropbox/Dissertation/Data Analysis/freq_Control')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-Token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Four steps:\n",
    "#1 Create a dictionary with file names as the keys and the chat text as values.\n",
    "#2 Separate native speaker and L2 learner dataset.\n",
    "#3 Extract the W + SFP constructions from the NS and L2 dataset, respectively.\n",
    "#4 Calculate the type-token frequency of W + SFP construction\n",
    "# Note: SFP: sentence final particles including a, ba, and ne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: \n",
    "# Create a dictionary: file name (key) --> text data (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dic(path):\n",
    "    # takes file path as input and return a dictionary \n",
    "    # with file name as key and the chat data as the value.\n",
    "    fname=glob.glob(path)\n",
    "    dic={}\n",
    "    for f in fname:\n",
    "        with open(f) as file:\n",
    "            data=file.read()\n",
    "        dic[f]=data\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the path or six chat sessions:\n",
    "path_wk1 = 'WK1*.txt'\n",
    "path_wk2 = 'WK2*.txt'\n",
    "path_wk3 = 'WK3*.txt'\n",
    "path_wk4 = 'WK4*.txt'\n",
    "path_wk5 = 'WK5*.txt'\n",
    "path_wk6 = 'WK6*.txt'\n",
    "path_all = 'WK*.txt'\n",
    "#get_dic(path_wk6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "# Separate NS and L2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NS_text(text):\n",
    "    # gets all the native speaker (NS) lines from a text\n",
    "    NS_text=''\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('NS'):\n",
    "            NS_text+=line\n",
    "    return NS_text\n",
    "\n",
    "def get_L2_text(text):\n",
    "    # gets all the L2 learner (L2) lines from a text\n",
    "    L2_text=''\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('L2'):\n",
    "            L2_text+=line\n",
    "    return L2_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "# Get SFPs a, ba, and ne from NS and L2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get W + a\n",
    "def get_a_NS(path):\n",
    "    # gets SFP a from NS datasets in each week\n",
    "    # the input path determines which week of data to analyze\n",
    "    dic = get_dic(path)\n",
    "    NS_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        NS_text+=get_NS_text(text)\n",
    "    li = re.findall('\\w{1,3}啊', NS_text)\n",
    "    return li\n",
    "#print(get_a_NS(path_wk6))\n",
    "\n",
    "def get_a_L2(path):\n",
    "    # gets SFP a from L2 datasets in each week\n",
    "    # the input path determines which week of data to analyze\n",
    "    dic = get_dic(path)\n",
    "    L2_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        L2_text+=get_L2_text(text)\n",
    "    li = re.findall('\\w{1,3}啊', L2_text)\n",
    "    return li\n",
    "#print(get_a_L2(path_wk6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get W + ba\n",
    "def get_ba_NS(path):\n",
    "    dic = get_dic(path)\n",
    "    NS_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        NS_text+=get_NS_text(text)\n",
    "    li = re.findall('\\w{1,3}吧', NS_text)\n",
    "    return li\n",
    "\n",
    "def get_ba_L2(path):\n",
    "    dic = get_dic(path)\n",
    "    L2_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        L2_text+=get_L2_text(text)\n",
    "    li = re.findall('\\w{1,3}吧', L2_text)\n",
    "    return li\n",
    "#print(get_ba_NS(path_wk6))\n",
    "#print(get_ba_L2(path_wk6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get W + ne\n",
    "def get_ne_NS(path):\n",
    "    dic = get_dic(path)\n",
    "    NS_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        NS_text+=get_NS_text(text)\n",
    "    li = re.findall('\\w{1,3}呢', NS_text)\n",
    "    return li\n",
    "\n",
    "def get_ne_L2(path):\n",
    "    dic = get_dic(path)\n",
    "    L2_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        L2_text+=get_L2_text(text)\n",
    "    li = re.findall('\\w{1,3}呢', L2_text)\n",
    "    return li\n",
    "#print(get_ne_NS(path_wk6))\n",
    "#print(get_ne_L2(path_wk6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: \n",
    "# Token frequency, type frequency, type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratio(li):\n",
    "    # takes a list and calculates its type-token ratio\n",
    "    ratio = len(set(li))/len(li)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of W + a from the NS dataset in each phase\n",
    "NS_a1=get_a_NS(path_wk1)+get_a_NS(path_wk2) # Phase 1\n",
    "NS_a2=get_a_NS(path_wk3)+get_a_NS(path_wk4) # Phase 2 \n",
    "NS_a3=get_a_NS(path_wk5)+get_a_NS(path_wk6) # Phase 3\n",
    "\n",
    "# Get a list of W + a from the L2 dataset in each phase\n",
    "L2_a1=get_a_L2(path_wk1)+get_a_L2(path_wk2)\n",
    "L2_a2=get_a_L2(path_wk3)+get_a_L2(path_wk4)\n",
    "L2_a3=get_a_L2(path_wk5)+get_a_L2(path_wk6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust the NS_a1 list. Remove the unnecessary words to get the W + a construction\n",
    "NS_a1_adjusted = ['搭配啊', '玩耍啊', '我啊', '是啊', '打算啊', '快啊', '好啊', '实习啊', '人啊', '这样啊', '重庆啊', '好啊', \n",
    "                  '酷啊', '棒啊', '不错啊', '对啊', '好啊', '了啊', '的啊', '研究啊', '星期天啊', '课啊', '课啊', '课啊', \n",
    "                  '关系啊', '文献啊', '看啊', '研究啊', '是啊', '你好啊', '你啊', '是啊', '感觉啊', '说法啊', '加油啊', \n",
    "                  '巴士啊', '人啊', '你好啊', '去啊', '春假啊', '可爱啊', '懵状啊', '什么啊', '对啊', '了啊', '做啊', '哪啊', \n",
    "                  '工作啊', '放假啊', '电视啊', '电影啊', '对啊', '好啊', '是啊', '对啊', '对啊', '对啊', '棒啊', '有啊', \n",
    "                  '学啊', '时候啊', '春假啊', '是啊', '你好啊', '学校啊', '不是啊']\n",
    "NS_a2_adjusted = ['你好啊', '地方啊', '什么啊', '这样啊', '是啊', '是啊', '是啊', '不同啊', '早啊', '了啊', '创意啊', '花园啊', \n",
    "                  '啥啊', '好啊', '圣诞节啊', '可爱啊', '人啊', '是啊', '没事啊', '是啊', '是啊', '是啊', '圣诞节啊', '什么样啊', \n",
    "                  '赞啊', '你好啊', '对啊', '干嘛啊', '好亏啊', '几天啊', '对啊', '方便啊', '多啊', '浙江啊', '苏州啊', '南京啊', \n",
    "                  '了啊', '峡谷啊', '公园啊', '了啊', '山珍啊', '野味啊', '对啊', '人啊', '的啊', '马桶啊', '张家界啊', '湖南啊', \n",
    "                  '难啊', '这样啊', '不错啊', '宝贵啊', '这样啊', '中文啊', '的啊', '棒啊', '衣服啊', '贵啊', '你好啊', '有缘啊', \n",
    "                  '是啊', '开心啊', '什么啊', '庆祝啊'] # 2 '是啊是啊‘ from the NS_a2 counted as 4 '是啊’ in the adjusted list\n",
    "NS_a3_adjusted = ['作业啊', '这样啊', '对啊', '是啊', '是啊', '尝尝啊', '你好啊', '识你啊', '丰富啊', '我想啊', '这样啊', \n",
    "                  '很棒啊', '意思啊', '好棒啊', '多久啊', '为啥啊', '不给啊', '活动啊', '很好啊', '不少啊', '识你啊', '新年啊', \n",
    "                  '辛苦啊', '厉害啊', '是啊', '是啊', '对啊', '哪里啊', '对啊', '对啊', '是啊', '对啊', '是啊', '多久啊', \n",
    "                  '有啊', '得妙啊', '是啊', '好啊', '好玩啊', '是啊', '网站啊', '记得啊', '旅游啊', '对啊', '对啊', '是你啊', \n",
    "                  '聊啊', '莱尔啊', '这样啊', '课啊', '圣诞节啊', '好奇啊', '火鸡啊', '几了啊', '上好啊', '丰富啊', '样子啊', \n",
    "                  '故事啊', '鞭炮啊', '这样啊'] # 2 '对啊对啊‘ from the NS_a2 counted as 4 '对啊’ in the adjusted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6818181818181818\n",
      "0.703125\n",
      "0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "# NS Type-token ratio of a in each phase\n",
    "NS_ratio_a1 = get_ratio(NS_a1_adjusted)\n",
    "print(NS_ratio_a1) # type-token ration in Phase 1\n",
    "NS_ratio_a2 = get_ratio(NS_a2_adjusted)\n",
    "print(NS_ratio_a2) # type-token ration in Phase 2\n",
    "NS_ratio_a3 = get_ratio(NS_a3_adjusted)\n",
    "print(NS_ratio_a3) # type-token ration in Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the L2_a1 list. Remove the unnecessary words to get the W + a construction\n",
    "L2_a1_adjusted = ['下雨啊', '长啊', '的啊', '天气啊', '问题啊', '天啊', '是啊', '知道啊', '开啊', '对啊', '好啊', '名字啊', \n",
    "                  '好啊', '厉害啊', '会啊', '要啊', '你好啊', '是啊', '万圣节啊', '高兴啊', '哪里啊', '是啊', '是啊', '是啊', \n",
    "                  '长时间啊', '是啊', '春季啊', '春假啊', '是啊', '是啊', '认识你啊', '是啊', '对啊', '厉害啊', '的啊',\n",
    "                  '有气氛啊', '天啊', '谢谢啊', '有意思啊', '火鸡啊', '是啊']\n",
    "L2_a2_adjusted = ['没事啊', '真的啊', '酷啊', '是啊', '是啊', '对啊', '穿啊', '对啊', '功课啊', '对啊', '是啊', '人啊', '好啊', \n",
    "                  '性啊', '忙啊', '怎么样啊', '漂亮啊', '对啊', '对啊', '对啊', '对啊', '是啊', '有趣啊', '是啊', 'cmu啊', '对啊', \n",
    "                  '对啊', '过啊', '蛋糕啊', '不错啊', '说啊', '不错啊', '你啊', '意思啊', '是啊', '对啊', '是啊']\n",
    "L2_a3_adjusted = ['是啊', '对啊', '好啊', '对啊', '糖啊', '高兴啊', '是啊', '是啊', '是啊', '专业啊', '你啊', '对啊', '没有啊', \n",
    "                  '对啊', '是啊', '是啊', '是啊', '长啊', '对啊', '对啊', '对啊', '哪儿啊', '真的啊', '对啊', '对啊', '疯啊', \n",
    "                  '他们啊', '实习啊', '是啊', '不是啊', '对啊', '不是啊', '慕森啊', '是啊', '不错啊', '是啊', '对啊']\n",
    "                    # removed 2 ‘啊啊啊’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6585365853658537\n",
      "0.5675675675675675\n",
      "0.4594594594594595\n"
     ]
    }
   ],
   "source": [
    "# L2 Type-token ratio of a in each phase\n",
    "L2_ratio_a1 = get_ratio(L2_a1_adjusted)\n",
    "print(L2_ratio_a1) # type-token ration in Phase 1\n",
    "L2_ratio_a2 = get_ratio(L2_a2_adjusted)\n",
    "print(L2_ratio_a2) # type-token ration in Phase 2\n",
    "L2_ratio_a3 = get_ratio(L2_a3_adjusted)\n",
    "print(L2_ratio_a3) # type-token ration in Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W + ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of W + ba from the NS dataset in each phase\n",
    "NS_ba1=get_ba_NS(path_wk1)+get_ba_NS(path_wk2) # Phase 1\n",
    "NS_ba2=get_ba_NS(path_wk3)+get_ba_NS(path_wk4) # Phase 2 \n",
    "NS_ba3=get_ba_NS(path_wk5)+get_ba_NS(path_wk6) # Phase 3 \n",
    "\n",
    "# Get a list of W + ba from the L2 dataset in each phase\n",
    "L2_ba1=get_ba_L2(path_wk1)+get_ba_L2(path_wk2)\n",
    "L2_ba2=get_ba_L2(path_wk3)+get_ba_L2(path_wk4)\n",
    "L2_ba3=get_ba_L2(path_wk5)+get_ba_L2(path_wk6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NS_ba1_adjusted = ['聊天吧', '的吧', '还好吧', '还好吧', '主题吧', '300km/h吧', '新年吧', '聊吧', '算大吧', '城市吧', '去吧', \n",
    "                   '久吧', '多吧', '好吧', '了吧', 'night吧', '小时吧', '春节吧', '小时吧', '什么吧', '开始吧', '懒觉吧', \n",
    "                   '的店吧', '酒店吧', '了吧', '有吧', '春假吧', '作业吧', '这里吧', '还行吧', '不会吧', '聊吧', '不够吧', \n",
    "                   '火鸡吧', '这里吧', '不错吧', '感觉吧'] # removed '奔跑吧', a TV show name\n",
    "NS_ba2_adjusted = ['匹兹堡吧', '远吧', '暑假吧', '暑假吧', '感恩节吧', '聊吧', '好吧', '庆祝吧', '主题吧', '是吧', '主题吧', \n",
    "                   '放假吧', '跨年吧', '没错吧', '还行吧', '了吧', '成都吧', '了吧', '看看吧', '到吧', '几天吧', 'Alto吧', \n",
    "                   '回家吧', '万圣节吧', '点吧', '节日吧', '多吧', 'Minor吧', '购物吧', '聊吧', '种类吧', '礼物吧', '电影吧', \n",
    "                   '还行吧']\n",
    "NS_ba3_adjusted = ['多吧', '忙吧', '计划吧', '好吧', '春假吧', '联系吧', '新年吧', '取暖吧', '时候吧', '对吧', '橄榄球吧', \n",
    "                   '老师吧', '不高吧', '难吧', '两周吧', '对吧', '万圣节吧', '了吧', '忙吧', '还行吧', '人吧', '过吧', '聊吧', \n",
    "                   '圣诞吧', '一样吧', '鱼吧', '的吧', '还好吧', '一趟吧', '聊吧', '多吧', '这里吧']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8648648648648649\n",
      "0.8529411764705882\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "# NS Type-token ratio of Ba in each phase\n",
    "NS_ratio_ba1 = get_ratio(NS_ba1_adjusted)\n",
    "print(NS_ratio_ba1) \n",
    "NS_ratio_ba2 = get_ratio(NS_ba2_adjusted)\n",
    "print(NS_ratio_ba2)\n",
    "NS_ratio_ba3 = get_ratio(NS_ba3_adjusted)\n",
    "print(NS_ratio_ba3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2_ba1_adjusted = ['的吧', '聊吧', '新年吧', '聊吧', '久吧', '学生吧', '买吧', 'bingchen吧', '电影吧', '狗吧', 'man吧', '对吧']\n",
    "L2_ba2_adjusted = ['东北吧', '东北吧', '好吧', '感觉吧', '欢车吧', '推荐吧']\n",
    "L2_ba3_adjusted = ['这样吧', '对吧', '感恩节吧', '这样吧', '人吧', '过吧', '匹兹堡吧']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "0.8333333333333334\n",
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# L2 Type-token ratio of ba in each phase\n",
    "L2_ratio_ba1 = get_ratio(L2_ba1_adjusted)\n",
    "print(L2_ratio_ba1)\n",
    "L2_ratio_ba2 = get_ratio(L2_ba2_adjusted)\n",
    "print(L2_ratio_ba2) \n",
    "L2_ratio_ba3 = get_ratio(L2_ba3_adjusted)\n",
    "print(L2_ratio_ba3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W + ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of W + ne from the NS dataset in each phase\n",
    "NS_ne1=get_ne_NS(path_wk1)+get_ne_NS(path_wk2) # Phase 1\n",
    "NS_ne2=get_ne_NS(path_wk3)+get_ne_NS(path_wk4) # Phase 2 \n",
    "NS_ne3=get_ne_NS(path_wk5)+get_ne_NS(path_wk6) # Phase 3 \n",
    "\n",
    "# Get a list of W + ne from the L2 dataset in each phase\n",
    "L2_ne1=get_ne_L2(path_wk1)+get_ne_L2(path_wk2)\n",
    "L2_ne2=get_ne_L2(path_wk3)+get_ne_L2(path_wk4)\n",
    "L2_ne3=get_ne_L2(path_wk5)+get_ne_L2(path_wk6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NS_ne1_adjusted = ['你呢', '你呢', '不错呢', '盛大呢', '明白呢', '是呢', '的呢', '了呢', '了呢', '飞快呢', '的呢', '学校呢', \n",
    "                   '你呢', '工作呢', '的呢', '你呢', '时间呢', '新年呢', '什么呢', '是呢', '什么呢', '地方呢', '地方呢', \n",
    "                   '东西呢', 'CMU呢', '的呢', '什么呢', '开车呢', '你呢', '考呢', '多久呢', '你呢', '你呢', '你呢', '什么课呢', \n",
    "                   '中文呢', '哪儿呢', '什么呢', '你呢', '方向呢', '项目呢', '你呢', '你呢', '的呢', '吃法呢', '味道呢', '你呢',\n",
    "                   '什么呢', '什么呢', '你呢', '哪里呢', '什么呢', '糖呢', '什么呢', '什么呢', '电影呢', '的呢', '你呢', '的呢',\n",
    "                   '的呢', '好听呢', '的呢', '的呢', '什么呢', '错觉呢', '是呢', '不错呢', '你呢', '概念呢', '时代剧呢', \n",
    "                   '时候呢', '你呢', '啥呢', '事情呢', '了呢', '你呢', '人呢', '聚会呢', '糖呢', 'off呢', '喜欢呢', '你呢', \n",
    "                   '你呢', '你呢', '教呢', '几月呢', '什么呢', '了呢', '比如呢', '你呢', '哪里呢', '你呢', '感恩节呢', '没有呢', \n",
    "                   '的呢', '然后呢']\n",
    "\n",
    "NS_ne2_adjusted = ['你呢', '暑假呢', '哪里呢', '做呢', '做呢', '的呢', '你呢', '节日呢', '比如呢', '节日呢', '你呢', '锅底呢', \n",
    "                   '菜呢', '者呢', '比如呢', '然后呢', '你呢', '你呢', '课呢', '你呢', '你呢', '的呢', '什么呢', '你呢', \n",
    "                   '出去呢', '什么呢', '电视呢', '什么呢', '新年呢', '你呢', '的呢', '什么呢', '的呢', '的呢', '计划呢', \n",
    "                   '考试呢', '你呢', '怎么样呢', '怎么样呢', '是呢', '烟花呢', '的呢', '不错呢', '确实呢', '的呢', '暑假呢', \n",
    "                   '干嘛呢', '生日呢', '的呢', '时间呢', '什么呢', '什么呢', '你呢', '的呢', '什么呢', '了呢', '的呢', '干嘛呢', \n",
    "                   '国家呢', '中文呢', '计划呢', '你呢', '什么呢', '是呢', '是呢', '什么呢', '是呢', '为什么呢', '是呢', \n",
    "                   '哪些呢', '礼物呢', '钱呢', '什么呢', '酒呢', '价格呢', '40刀呢', '的呢', '的呢', '过呢', '电影呢', '你呢', \n",
    "                   '你呢', '什么呢', '礼物呢', '什么呢', '什么呢', '什么呢', '了呢', '人呢', '着呢', '多久呢', '晚会呢']\n",
    "\n",
    "NS_ne3_adjusted = ['你呢', '年呢', '决定呢', '你呢', '你呢', '你呢', '什么呢', '哪儿呢', '州呢', '什么呢', '的呢', '菜呢', '菜呢',\n",
    "                   '什么呢', '玩儿呢', '知道呢', '春假呢', '是呢', '人呢', '人呢', '的呢', '研究生呢', '匹兹堡呢', '你呢', '的呢', \n",
    "                   '的呢', '嗯呢', '庆祝呢', '什么呢', '棒呢', '烟花呢', '嗯呢', '你呢', '的呢', '觉得呢', '你呢', '什么呢', \n",
    "                   '专业呢', '的呢', '哪里呢', '哪里呢', '工作呢', '活动呢', '你呢', '什么呢', '了呢', '巧克力呢', '专业呢', \n",
    "                   '你呢', '什么呢', '方面呢', '时间呢', '工作呢', '然后呢', '哪里呢', '圣诞节呢', '过呢', '什么呢', '国家呢', \n",
    "                   '庆呢', '你呢', '你呢', '滑雪呢', '多久呢', '难怪呢', '中国呢', '家呢', '你呢', '不呢', '的呢', '嗯呢', '了呢', \n",
    "                   '匹兹堡呢', '你呢', '访问呢','的呢', '的呢', '你呢', '你呢', '知道呢', '过呢', '上学呢', '这样子呢', '哪些呢', \n",
    "                   '什么呢', '为什么呢', '的呢', '的呢', '礼物呢', '好呢', '冷呢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4791666666666667\n",
      "0.45652173913043476\n",
      "0.5164835164835165\n"
     ]
    }
   ],
   "source": [
    "# NS Type-token ratio of ne in each phase\n",
    "NS_ratio_ne1 = get_ratio(NS_ne1_adjusted)\n",
    "print(NS_ratio_ne1) \n",
    "NS_ratio_ne2 = get_ratio(NS_ne2_adjusted)\n",
    "print(NS_ratio_ne2)\n",
    "NS_ratio_ne3 = get_ratio(NS_ne3_adjusted)\n",
    "print(NS_ratio_ne3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_ne1_adjusted = ['你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '多呢', '多久呢', '觉得呢', '你呢', '你呢', '什么呢', '你呢', \n",
    "                   '你呢', '你们呢', '年级呢', '土豆泥呢', '你呢', '你呢', '吃呢', '到呢', '你呢', '你呢', '你呢', '你呢', \n",
    "                   '你呢', '你呢', '你呢', '开始呢', '什么呢', '你呢', '你呢', '你呢', '你呢', '你呢', '春假呢', '你呢']\n",
    "L2_ne2_adjusted = ['工作呢', '的呢', '你呢', '你呢', '实习呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '什么呢', \n",
    "                   '你呢', '你呢', '觉得呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '干嘛呢', '你呢', \n",
    "                   '你呢', '元旦呢', '为什么呢', '你呢', '上学呢', '你呢', '你呢', '你呢', '知道呢', '你呢', '你呢']\n",
    "L2_ne3_adjusted = ['你呢', '个呢', '你呢', '了呢', '你呢', '你呢', '你呢', '你呢', '你呢', '的呢', '你呢', '你呢', '你呢', \n",
    "                   '在呢', '停留呢', '你呢', '你呢', '你呢', '你呢', '度假呢', '你呢', '你呢', '你呢', '那儿呢', '你呢', \n",
    "                   '过呢', '什么呢', '你呢', '你呢', '你呢', '现在呢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32432432432432434\n",
      "0.3055555555555556\n",
      "0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "# L2 Type-token ratio of ne in each phase\n",
    "L2_ratio_ne1 = get_ratio(L2_ne1_adjusted)\n",
    "print(L2_ratio_ne1) \n",
    "L2_ratio_ne2 = get_ratio(L2_ne2_adjusted)\n",
    "print(L2_ratio_ne2)\n",
    "L2_ratio_ne3 = get_ratio(L2_ne3_adjusted)\n",
    "print(L2_ratio_ne3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contingency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 NS dataset: Get all the W + SFP constructions (all SFPs all phases in one object)\n",
    "#2 NS dataset: Creat a dictionary with W + SFP as the key and its frequency as the value \n",
    "                # -> for contingency calculation\n",
    "#3 L2 datasets: Get a dictionary with W + SFP as the key and its frequency as the value \n",
    "                # for each SFP in each phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "# All W + SFP constructions from the NS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 103 279\n"
     ]
    }
   ],
   "source": [
    "# get all the a, ba, and ne from the three phases\n",
    "All_NS_a = NS_a1_adjusted + NS_a2_adjusted + NS_a3_adjusted\n",
    "All_NS_ba = NS_ba1_adjusted + NS_ba2_adjusted + NS_ba3_adjusted\n",
    "All_NS_ne = NS_ne1_adjusted + NS_ne2_adjusted + NS_ne3_adjusted\n",
    "\n",
    "# Token frequency of W + a, ba, and ne\n",
    "print(len(All_NS_a), len(All_NS_ba), len(All_NS_ne))\n",
    "\n",
    "# get all the SFPs used by the NS in the control group\n",
    "All_NS_SFPs = All_NS_a + All_NS_ba + All_NS_ne\n",
    "# print(sorted(All_NS_SFPs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: \n",
    "# Type frequency of W + SFP in the NS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContruction_dic(a_list):\n",
    "    # takes a list as input and returns a dictionary with list elements as the key \n",
    "    # and the elements' frequency as values\n",
    "    result_dic = {}\n",
    "    for i in a_list:\n",
    "        if i in result_dic:\n",
    "            result_dic[i]+=1\n",
    "        else:\n",
    "            result_dic[i]=1\n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300km/h吧:1\n",
      "40刀呢:1\n",
      "Alto吧:1\n",
      "CMU呢:1\n",
      "Minor吧:1\n"
     ]
    }
   ],
   "source": [
    "# get type frequency of W + SFP\n",
    "All_NS_SFPs_dic = getContruction_dic(All_NS_SFPs)\n",
    "for i in sorted(All_NS_SFPs_dic.keys())[0:5]:\n",
    "    print(\"%s:%s\"% (i, All_NS_SFPs_dic[i])) # formatting: W + SFP: frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: \n",
    "# Frequency of W + SFP in the L2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'下雨啊': 1, '长啊': 1, '的啊': 2, '天气啊': 1, '问题啊': 1, '天啊': 2, '是啊': 10, '知道啊': 1, '开啊': 1, '对啊': 2, '好啊': 2, '名字啊': 1, '厉害啊': 2, '会啊': 1, '要啊': 1, '你好啊': 1, '万圣节啊': 1, '高兴啊': 1, '哪里啊': 1, '长时间啊': 1, '春季啊': 1, '春假啊': 1, '认识你啊': 1, '有气氛啊': 1, '谢谢啊': 1, '有意思啊': 1, '火鸡啊': 1}\n",
      "{'没事啊': 1, '真的啊': 1, '酷啊': 1, '是啊': 7, '对啊': 10, '穿啊': 1, '功课啊': 1, '人啊': 1, '好啊': 1, '性啊': 1, '忙啊': 1, '怎么样啊': 1, '漂亮啊': 1, '有趣啊': 1, 'cmu啊': 1, '过啊': 1, '蛋糕啊': 1, '不错啊': 2, '说啊': 1, '你啊': 1, '意思啊': 1}\n",
      "{'是啊': 10, '对啊': 11, '好啊': 1, '糖啊': 1, '高兴啊': 1, '专业啊': 1, '你啊': 1, '没有啊': 1, '长啊': 1, '哪儿啊': 1, '真的啊': 1, '疯啊': 1, '他们啊': 1, '实习啊': 1, '不是啊': 2, '慕森啊': 1, '不错啊': 1}\n"
     ]
    }
   ],
   "source": [
    "# frequency of W + a in each phase\n",
    "L2_a1_dic = getContruction_dic(L2_a1_adjusted)\n",
    "print(L2_a1_dic)\n",
    "L2_a2_dic = getContruction_dic(L2_a2_adjusted)\n",
    "print(L2_a2_dic)\n",
    "L2_a3_dic = getContruction_dic(L2_a3_adjusted)\n",
    "print(L2_a3_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of W + ba in each phase\n",
    "L2_ba1_dic = getContruction_dic(L2_ba1_adjusted)\n",
    "# print(L2_ba1_dic)\n",
    "L2_ba2_dic = getContruction_dic(L2_ba2_adjusted)\n",
    "# print(L2_ba2_dic)\n",
    "L2_ba3_dic = getContruction_dic(L2_ba3_adjusted)\n",
    "# print(L2_ba3_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of W + ne in each phase\n",
    "L2_ne1_dic = getContruction_dic(L2_ne1_adjusted)\n",
    "# L2_ne1_dic\n",
    "L2_ne2_dic = getContruction_dic(L2_ne2_adjusted)\n",
    "# L2_ne2_dic\n",
    "L2_ne3_dic = getContruction_dic(L2_ne3_adjusted)\n",
    "# L2_ne3_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
