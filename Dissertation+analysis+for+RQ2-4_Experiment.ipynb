{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/qiongli/Dropbox/Dissertation/Data Analysis/freq_Experiment')\n",
    "#os.getcwd()\n",
    "#glob.glob('WK6*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Four steps:\n",
    "#1 Create a dictionary with file names as the keys and the chat text as values.\n",
    "#2 Separate native speaker and L2 learner dataset.\n",
    "#3 Extract the W + SFP constructions from the NS and L2 dataset, respectively.\n",
    "#4 Calculate the type-token frequency of W + SFP construction\n",
    "# Note: SFP: sentence final particles including a, ba, and ne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: \n",
    "# Create a dictionary: file name (key) --> text data (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dic(path):\n",
    "    # takes file path as input and return a dictionary \n",
    "    # with file name as key and the chat data as the value.\n",
    "    fname=glob.glob(path)\n",
    "    dic={}\n",
    "    for f in fname:\n",
    "        with open(f) as file:\n",
    "            data=file.read()\n",
    "        dic[f]=data\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define all the path or six chat sessions:\n",
    "path_wk1 = 'WK1*.txt'\n",
    "path_wk2 = 'WK2*.txt'\n",
    "path_wk3 = 'WK3*.txt'\n",
    "path_wk4 = 'WK4*.txt'\n",
    "path_wk5 = 'WK5*.txt'\n",
    "path_wk6 = 'WK6*.txt'\n",
    "path_all = 'WK*.txt'\n",
    "# get_dic(path_wk6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "# Separate NS and L2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NS_text(text):\n",
    "    # gets all the native speaker (NS) lines from a text\n",
    "    NS_text=''\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('NS'):\n",
    "            NS_text+=line\n",
    "    return NS_text\n",
    "\n",
    "def get_L2_text(text):\n",
    "    # gets all the L2 learner (L2) lines from a text\n",
    "    L2_text=''\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('L2'):\n",
    "            L2_text+=line\n",
    "    return L2_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "# Get SFPs a, ba, and ne from NS and L2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get W + a\n",
    "def get_a_NS(path):\n",
    "    # gets SFP a from NS datasets in each week\n",
    "    # the input path determines which week of data to analyze\n",
    "    dic = get_dic(path)\n",
    "    NS_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        NS_text+=get_NS_text(text)\n",
    "    li = re.findall('\\w{1,3}啊', NS_text)\n",
    "    return li\n",
    "#print(get_a_NS(path_wk6))\n",
    "\n",
    "def get_a_L2(path):\n",
    "    # gets SFP a from L2 datasets in each week\n",
    "    # the input path determines which week of data to analyze\n",
    "    dic = get_dic(path)\n",
    "    L2_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        L2_text+=get_L2_text(text)\n",
    "    li = re.findall('\\w{1,3}啊', L2_text)\n",
    "    return li\n",
    "#print(get_a_L2(path_wk6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get W + ba\n",
    "def get_ba_NS(path):\n",
    "    dic = get_dic(path)\n",
    "    NS_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        NS_text+=get_NS_text(text)\n",
    "    li = re.findall('\\w{1,3}吧', NS_text)\n",
    "    return li\n",
    "\n",
    "def get_ba_L2(path):\n",
    "    dic = get_dic(path)\n",
    "    L2_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        L2_text+=get_L2_text(text)\n",
    "    li = re.findall('\\w{1,3}吧', L2_text)\n",
    "    return li\n",
    "#print(get_ba_NS(path_wk6))\n",
    "#print(get_ba_L2(path_wk6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get W + ne\n",
    "def get_ne_NS(path):\n",
    "    dic = get_dic(path)\n",
    "    NS_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        NS_text+=get_NS_text(text)\n",
    "    li = re.findall('\\w{1,3}呢', NS_text)\n",
    "    return li\n",
    "\n",
    "def get_ne_L2(path):\n",
    "    dic = get_dic(path)\n",
    "    L2_text=''\n",
    "    for key in dic.keys():\n",
    "        text=dic[key]\n",
    "        L2_text+=get_L2_text(text)\n",
    "    li = re.findall('\\w{1,3}呢', L2_text)\n",
    "    return li\n",
    "#print(get_ne_NS(path_wk6))\n",
    "#print(get_ne_L2(path_wk6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: \n",
    "# Token frequency, type frequency, type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratio(li):\n",
    "    # takes a list and calculates its type-token ratio\n",
    "    ratio = len(set(li))/len(li)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of W + a from the NS dataset in each phase\n",
    "NS_a1=get_a_NS(path_wk1)+get_a_NS(path_wk2) # Phase 1\n",
    "NS_a2=get_a_NS(path_wk3)+get_a_NS(path_wk4) # Phase 2 \n",
    "NS_a3=get_a_NS(path_wk5)+get_a_NS(path_wk6) # Phase 3 \n",
    "\n",
    "# Get a list of W + a from the L2 dataset in each phase\n",
    "L2_a1=get_a_L2(path_wk1)+get_a_L2(path_wk2)\n",
    "L2_a2=get_a_L2(path_wk3)+get_a_L2(path_wk4)\n",
    "L2_a3=get_a_L2(path_wk5)+get_a_L2(path_wk6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust the NS_a1 list. Remove the unnecessary words to get the W + a construction\n",
    "NS_a1_adjusted = ['好啊', '期待啊', '羡慕啊', '棒啊', '这样啊', '吓人啊', '这样啊', '名字啊', '是啊', \n",
    "                  '名字啊', '好啊', '什么啊', '对啊', '了啊', '我啊', '难啊', '词汇啊', '语法啊', \n",
    "                  '不错啊', '麻烦啊', '淇淋啊', '淇淋啊', '的啊', '公司啊', '什么啊', '对啊', '聚会啊', \n",
    "                  '船啊', '正常啊', '是啊', '是啊', '厉害啊', '意思啊', '有啊', '菜啊', '做法啊', \n",
    "                  '我啊', '这样啊', '愿者啊', '会啊', '不错啊', '专业啊', 've啊', '对啊', '的啊', \n",
    "                  '辣啊', '鸭子啊', '的啊', '好啊', '漂亮啊', '对啊', '同意啊', '是啊', '天啊', '学习啊', \n",
    "                  '是啊', '什么啊', '尴尬啊', '的啊', '月饼啊', '这样啊', '大餐啊', '北方啊', 'ct啊', 'ne啊', '这样啊'] # '对啊对啊‘ counted as 1 in the NS_a1 list, but counted as 2 in NS_a1_adjusted list\n",
    "\n",
    "NS_a2_adjusted = ['的啊', '巧啊', '这样啊', '哪里啊', '是啊', '是啊', '是啊', '再见啊', '真的啊', '的啊', '了啊', \n",
    "                  '是啊', '好啊', '棒啊', '孤独啊', '什么啊', '礼物啊', '是啊', '的啊', '纽约啊', '的啊', '怎么样啊', \n",
    "                  '年级啊', '的啊', '棒啊', '是啊', '有意思啊', '有意思啊', '春天啊', '是啊', '天啊', '知道啊', '对啊', \n",
    "                  '对啊', '熊猫啊', '早啊', '贝啊', '见啊', '周啊', '了啊', '哪有啊', '春假啊', '哪儿啊', '回去啊', \n",
    "                  '的啊', '活动啊', '近啊', '身体啊', '多少啊', '实验啊', '这样啊', '有趣啊', '知道啊', '哪里啊', \n",
    "                  '节啊', '什么啊', '的啊', '的啊'] # '对啊对啊‘ counted as 1 in the NS_a2 list, but counted as 2 in NS_a2_adjusted list\n",
    "\n",
    "NS_a3_adjusted = ['圣诞节啊', '歌啊', '年级啊', '一起啊', '这样啊', '哪儿啊', '哪儿啊', '繁华啊', '的啊', '好啊', '对啊', \n",
    "                  '缘分啊', '中文啊', '知道啊', '节目啊', '的啊', '小说啊', '文歌啊', '聊啊', '好啊', '辛苦啊', '苏州啊', \n",
    "                  '一下啊', '这样啊', 'll啊', '感恩节啊', '喜欢啊', '什么啊', '的啊', '这样啊', '这样啊', '这样啊', '有才啊', \n",
    "                  '多啊', '天啊', '妆啊', '是啊', '是啊', '童趣啊', '期待啊', '我啊', '化妆啊', '南瓜啊', '考试啊', 'ts啊', \n",
    "                  '什么啊', '什么啊', '这样啊', '我啊', '地道啊', '羡慕啊', '火锅啊', '聊啊', '你好啊', '期待啊', '好奇啊', \n",
    "                  '没有啊', '知道啊', '这样啊', '喜欢啊', '再见啊', '忙啊', '人啊', '鬼啊', '鬼片啊', '这样啊', '做啊','什么啊', \n",
    "                  '吃啊', '这样啊', '特别啊', '庆祝啊', '什么啊', '当然啊', '意思啊', '是啊', '会啊', '道具啊', '会啊'] # dropped '啊啊', '呃啊' from the NS_a3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.6379310344827587\n",
      "0.6962025316455697\n"
     ]
    }
   ],
   "source": [
    "# NS Type-token ratio of a in each phase\n",
    "NS_ratio_a1 = get_ratio(NS_a1_adjusted)\n",
    "print(NS_ratio_a1) # type-token ration in Phase 1\n",
    "NS_ratio_a2 = get_ratio(NS_a2_adjusted)\n",
    "print(NS_ratio_a2) # type-token ration in Phase 2\n",
    "NS_ratio_a3 = get_ratio(NS_a3_adjusted)\n",
    "print(NS_ratio_a3) # type-token ration in Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['是的啊', '年级啊', '决定啊', '这样子啊', '照片啊', '谢谢啊', '是啊', '你啊', '是啊', '有意思啊', '是啊', '是啊', '对啊', '城市啊', '是啊', '安排啊', '好玩啊', '是啊', '对啊', '对啊', '是的啊', '有意思啊', '对啊', '没错啊', '好啊', '对啊', '是啊', '对啊', '好啊', '对不起啊', '好啊', '好吃啊', '忙啊', '知道啊', '是啊', '对啊', '城市啊', '是啊', '嗯对啊', '好吃啊', '是啊', '是啊', '对啊', '是啊', '北方啊', '是啊', '火锅啊', '香啊', '好吃啊', '试啊', '好啊', '是啊', '是啊', 'ht啊', '好玩啊', '这样啊', '万圣节啊', '有意思啊', '对啊', '错啊', '的啊', '是啊', '是啊', '厉害啊', '好啊', '好啊', '有意思啊', '好啊', '样子啊', '对啊']\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "# adjust the L2_a1 list. Remove the unnecessary words to get the W + a construction\n",
    "L2_a1_adjusted = ['是啊', '这样啊', '可怜啊', '对啊', '对啊', '对啊', '是啊', '是啊', '是啊', '好看啊', '是啊', \n",
    "                  '是啊', '对啊', '是啊', '对啊', '对啊', '对啊', '难啊', '好啊', '对啊', '好啊', '好啊', '是啊', \n",
    "                  '是啊', '对啊', '的啊', '对啊', '对啊', '了啊', '好啊', '有趣啊', '好啊', '冰岛啊', '那里啊', '好吃啊', \n",
    "                  '对啊', '父母啊', '可爱啊', '烂漫啊', '好啊', '好吃啊', '对啊', '好啊'] # removed 6 '啊啊' from L2_a1 list\n",
    "\n",
    "L2_a2_adjusted = ['没有啊', '够了啊', '可怜啊', '工作啊', '是啊', '对啊', '对啊', '是啊', '好吃啊', '是啊', '对啊', '好看啊', \n",
    "                  '确实啊', '人啊', '对啊', '嗯对啊', 'AC啊', '对啊', '好啊', '中国啊', '对啊', '热心啊', '对啊', '对不起啊', \n",
    "                  '难啊', '难啊', '嗯对啊', '好啊', '是啊', '难啊', '意思啊', '对啊', '独子啊', '对啊', '不起啊', '对啊', '对啊',\n",
    "                  '对啊', '对啊', '对啊', '好啊', '好玩啊', '喜欢啊', '是啊', '对啊', '好吃啊', '好吃啊', '聊天啊', '收到啊', \n",
    "                  '没有啊', '对啊', '好啊', '有啊', '是啊', '是的啊', '是啊', '是啊', '电池啊', '是啊', '对啊', '厉害啊', \n",
    "                  '厉害啊', '懂行啊', '好啊', '难啊', '对啊', '喜欢啊', '好啊', '对啊', '可怜啊', '糟糕啊', '有趣啊', '熊猫啊', \n",
    "                  '可惜啊', '是啊', '对啊', '有意思啊', '那样啊', '那好啊', '对啊', '好啊', '对啊', '对啊', '对啊', '对啊', \n",
    "                  '没错啊', '忙啊', '没有啊', '了啊', '对啊', '对啊', '听过啊', '对啊', '好啊', '养啊', '对啊', '是啊', '是啊', \n",
    "                  '对啊', '好啊', '是啊', '好吃啊', '鱼啊', '鱼啊', '不错啊'] # remived '啊啊', '哈哈啊' from L2_a2\n",
    "\n",
    "L2_a3_adjusted = ['是的啊', '年级啊', '决定啊', '这样子啊', '照片啊', '谢谢啊', '是啊', '你啊', '是啊', '有意思啊', '是啊', '是啊',\n",
    "                  '对啊', '城市啊', '是啊', '安排啊', '好玩啊', '是啊', '对啊', '对啊', '是的啊', '有意思啊', '对啊', '没错啊', \n",
    "                  '好啊', '对啊', '是啊', '对啊', '好啊', '对不起啊', '好啊', '好吃啊', '忙啊', '知道啊', '是啊', '对啊', \n",
    "                  '城市啊', '是啊', '嗯对啊', '好吃啊', '是啊', '是啊', '对啊', '是啊', '北方啊', '是啊', '火锅啊', '香啊', \n",
    "                  '好吃啊', '试啊', '好啊', '是啊', '是啊', 'ht啊', '好玩啊', '这样啊', '万圣节啊', '有意思啊', '对啊', '错啊', \n",
    "                  '的啊', '是啊', '是啊', '厉害啊', '好啊', '好啊', '有意思啊', '好啊', '样子啊', '对啊'] \n",
    "                    # removed 9 of '啊啊', '哈哈啊', '嗯啊’ \n",
    "print(L2_a3_adjusted)\n",
    "print(len(L2_a3_adjusted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37209302325581395\n",
      "0.4095238095238095\n",
      "0.44285714285714284\n"
     ]
    }
   ],
   "source": [
    "# L2 Type-token ratio of a in each phase\n",
    "L2_ratio_a1 = get_ratio(L2_a1_adjusted)\n",
    "print(L2_ratio_a1) # type-token ration in Phase 1\n",
    "L2_ratio_a2 = get_ratio(L2_a2_adjusted)\n",
    "print(L2_ratio_a2) # type-token ration in Phase 2\n",
    "L2_ratio_a3 = get_ratio(L2_a3_adjusted)\n",
    "print(L2_ratio_a3) # type-token ration in Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W + ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of W + ba from the NS dataset in each phase\n",
    "NS_ba1=get_ba_NS(path_wk1)+get_ba_NS(path_wk2) # Phase 1\n",
    "NS_ba2=get_ba_NS(path_wk3)+get_ba_NS(path_wk4) # Phase 2 \n",
    "NS_ba3=get_ba_NS(path_wk5)+get_ba_NS(path_wk6) # Phase 3 \n",
    "\n",
    "# Get a list of W + ba from the L2 dataset in each phase\n",
    "L2_ba1=get_ba_L2(path_wk1)+get_ba_L2(path_wk2)\n",
    "L2_ba2=get_ba_L2(path_wk3)+get_ba_L2(path_wk4)\n",
    "L2_ba3=get_ba_L2(path_wk5)+get_ba_L2(path_wk6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NS_ba1_adjusted = ['图片吧', '好吧', '新年吧', '假期吧', '聊吧', '圣诞节吧', '这里吧', '远吧', '通知吧', '巧克力吧', '春假吧', \n",
    "                   '对吧', '聊吧', '好吧', '多吧', '事情吧', '年级吧', '月吧', '也许吧', '情况吧', 'er吧', '觉吧', '顿饭吧', \n",
    "                   '啥吧', '开始吧', '菜吧', '开始吧', '北方吧', '暑假吧', '经历吧', '过吧', '开始吧', '棒吧', '一样吧', '这吧', \n",
    "                   '春假吧', '学生吧', '美吧', '认识吧', '这吧']\n",
    "\n",
    "NS_ba2_adjusted = ['春假吧', '春假吧', '漂亮吧', '加州吧', '对吧', '了吧', '好吧', '人们吧', 'hi吧', '好吧', '细节吧', '饭吧', \n",
    "                   '聚吧', '还好吧', '计划吧', '艺术吧', '了吧', '什么吧', '会吧', '半吧', '聊吧', '是吧', '这里吧', '时间吧', \n",
    "                   '回去吧', '庆祝吧', '个吧', '作业吧', '这样吧', '元旦吧', '幸福吧', '这样吧', '开始吧', '新年吧', '吃饭吧', \n",
    "                   '好吧', '实习吧', '逛逛吧', '经历吧', '这里吧', '感恩节吧', '远吧'] # removed 2 ‘酒吧'\n",
    "\n",
    "NS_ba3_adjusted = ['开始吧', '对吧', '正宗吧', '男票吧', '忙吧', '了吧', '社团吧', '是的吧', '新年吧', '春假吧', '多吧', \n",
    "                   '一样吧', '没有吧', '月吧', '住吧', '诞节吧', '万圣节吧', '对吧', '娃娃吧', '努力吧', '星期吧', '这吧', \n",
    "                   '好吧', '还行吧', '多吧', '这里吧', '回家吧', '新年吧', '过去吧', '近吧', '不过吧', '火吧', '东西吧', \n",
    "                   '聊吧', '对吧', '个吧', '忙吧', '圣诞节吧', '天吧', '天吧', '课吧', '不错吧', '好吃吧', '多吧', '派对吧']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "0.8571428571428571\n",
      "0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "# NS Type-token ratio of Ba in each phase\n",
    "NS_ratio_ba1 = get_ratio(NS_ba1_adjusted)\n",
    "print(NS_ratio_ba1) \n",
    "NS_ratio_ba2 = get_ratio(NS_ba2_adjusted)\n",
    "print(NS_ratio_ba2)\n",
    "NS_ratio_ba3 = get_ratio(NS_ba3_adjusted)\n",
    "print(NS_ratio_ba3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2_ba1_adjusted = ['走吧', '前吧', '小时吧', '到吧', '好吧', '对吧', '好吧', '好吧', '了吧', '没有吧', '可以吧', '好吧', \n",
    "                   '去吧', '午饭吧', '可以吧']\n",
    "\n",
    "L2_ba2_adjusted = ['春假吧', '城市吧', '这里吧', '是吧', '聊吧', '好吧', '研究生吧', '上学吧', '好吧', '好吧', '好吧', '好吧', \n",
    "                   '好吧', '好吧', '学吧', '行吧', '定吧', '一吧', '忙吧', '韩国吧', '回家吧', '收到吧', '点吧', '好吧', \n",
    "                   '好吧', '衣服吧', '好吧', '很好吧', '好吧', '好吧', '暑假吧', '好玩吧', '多吧', '暑假吧', '清淡吧']\n",
    "\n",
    "L2_ba3_adjusted = ['韩国吧', '好吧', '好吧', '对吧', '新年吧', '红包吧', '好吧', '传统吧', '这里吧', '这个吧', '景色吧', '行吧', \n",
    "                   '海边吧', '70吧', '是吧', '了吧', '好吧', '好吧']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7333333333333333\n",
      "0.6571428571428571\n",
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# L2 Type-token ratio of ba in each phase\n",
    "L2_ratio_ba1 = get_ratio(L2_ba1_adjusted)\n",
    "print(L2_ratio_ba1)\n",
    "L2_ratio_ba2 = get_ratio(L2_ba2_adjusted)\n",
    "print(L2_ratio_ba2) \n",
    "L2_ratio_ba3 = get_ratio(L2_ba3_adjusted)\n",
    "print(L2_ratio_ba3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W + ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of W + ne from the NS dataset in each phase\n",
    "NS_ne1=get_ne_NS(path_wk1)+get_ne_NS(path_wk2) # Phase 1\n",
    "NS_ne2=get_ne_NS(path_wk3)+get_ne_NS(path_wk4) # Phase 2 \n",
    "NS_ne3=get_ne_NS(path_wk5)+get_ne_NS(path_wk6) # Phase 3 \n",
    "\n",
    "# Get a list of W + ne from the L2 dataset in each phase\n",
    "L2_ne1=get_ne_L2(path_wk1)+get_ne_L2(path_wk2)\n",
    "L2_ne2=get_ne_L2(path_wk3)+get_ne_L2(path_wk4)\n",
    "L2_ne3=get_ne_L2(path_wk5)+get_ne_L2(path_wk6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NS_ne1_adjusted = ['你呢', '好的呢', '嗯呢', '你呢', '嗯呢', '事情呢', '的呢', '什么呢', '新年呢', '了呢', '什么呢', '了呢', \n",
    "                   '开心呢', '的呢', '冷呢', '时候呢', '哪儿呢', '了呢', '什么呢', '你呢', '你呢', '为什么呢', '什么呢', \n",
    "                   '东西呢', '玩呢', '你呢', '工作呢', '人物呢', '你们呢', '名字呢', '救生员呢', '你呢', '钱呢', '怎么样呢', \n",
    "                   '过呢', '你呢', '什么呢', '什么呢', '的呢', '为什么呢', '关系呢', '看呢', '年代呢', '年代呢', '然后呢', \n",
    "                   '调料呢', '牌子呢', '在呢', '你呢', '的呢', '的呢', '是呢', '看呢', '的呢', '好呢', '你呢', '人呢', '没有呢',\n",
    "                   '什么呢', '玩呢', '怎么样呢', '你呢', '什么呢', '办呢', '玩儿呢', '衣服呢', '你呢', '你呢', '什么呢', '什么呢',\n",
    "                   '多久呢', '你呢', '什么呢', '你呢', '穿呢', '装扮呢', '的呢', '时候呢', '你呢', '你呢', '你呢', '你呢', \n",
    "                   '你呢','你呢', '你呢', '你呢', '棒呢', '你呢', '好的呢', '的呢', '什么呢', '幸福呢', '天呢', '餐厅呢', \n",
    "                   '了呢', '假呢', '了呢', '哪些呢']\n",
    "\n",
    "NS_ne2_adjusted = ['什么呢', '的呢', '你呢', '什么呢', '什么呢', '你呢', '你呢', '什么呢', '什么呢', '你呢', '好呢', '学期呢', \n",
    "                   '过呢', '加州呢', '好呢', '你呢', '你呢', '什么呢', '你呢', 'ent呢', '新年呢', '今年呢', '好的呢', \n",
    "                   '万圣节呢', '你呢', '什么呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '的呢', \n",
    "                   '了呢', '好听呢', '为什么呢', '季节呢', '语言呢', '什么呢', '了呢', '了呢', '说呢', '专业呢', '干嘛呢', \n",
    "                   '你呢', '年呢', '食物呢', '什么呢', '感恩节呢', '你呢', '了呢', '不少呢', '好呢', '了呢', '你呢', '什么呢', \n",
    "                   '不知道呢', '你呢', '哪里呢', '的呢', '春假呢', '的呢', '电视呢', '干什么呢', '什么呢', '你呢', '兴趣呢', \n",
    "                   '题目呢', '什么呢', '你呢', '你呢', '今年呢', '课呢', '课呢', '棒呢', '你呢', '游戏呢', '你呢']\n",
    "\n",
    "NS_ne3_adjusted = ['你呢', '你们呢', '年轻呢', '你呢', '着呢', '你呢', '菜呢', '你呢', '多呢', '是的呢', '是呢', '人文呢', \n",
    "                   '哪里呢', '多久呢', '上班呢', '春假呢', '什么呢', '你呢', '什么呢', '菜呢', '开心呢', '嗯呢', '是呢', \n",
    "                   '什么呢', '公司呢', '你呢', '的呢', '你呢', '菜呢', '活动呢', '假期呢', '活动呢', '的呢', '你呢', '玩呢', \n",
    "                   '哪里呢', '在呢', '计划呢', '装扮呢', '你呢', '什么呢', '礼物呢', '的呢', '你呢', '没有呢', '过呢', '短呢', \n",
    "                   '你呢', '过呢', '一些呢', '美丽呢', '地方呢', '你呢', '了呢', '的呢', '了呢', '是的呢', '没有呢', '期待呢', \n",
    "                   '感恩节呢', '说过呢', '你呢', '过呢', '什么呢', '什么呢', '做呢', '什么呢', '下来呢', '传统呢', '的呢', \n",
    "                   '南瓜呢', '糖呢', '巧克力呢', '你呢', '喜欢呢', '的呢', '说呢', '礼物呢', '什么呢', '你呢', '什么呢', '你呢', \n",
    "                   '的呢', '什么呢', '加班呢', '你呢', '你呢', '什么呢', '什么呢', '干嘛呢', '礼物呢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46938775510204084\n",
      "0.425\n",
      "0.4945054945054945\n"
     ]
    }
   ],
   "source": [
    "# NS Type-token ratio of ne in each phase\n",
    "NS_ratio_ne1 = get_ratio(NS_ne1_adjusted)\n",
    "print(NS_ratio_ne1) \n",
    "NS_ratio_ne2 = get_ratio(NS_ne2_adjusted)\n",
    "print(NS_ratio_ne2)\n",
    "NS_ratio_ne3 = get_ratio(NS_ne3_adjusted)\n",
    "print(NS_ratio_ne3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2_ne1_adjusted = ['你呢', '什么呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', \n",
    "                   '你呢', '你呢', '你呢', '你呢', '你呢', '知道呢', '你呢', '你呢', '你呢', '你呢', '你呢', '你呢', '大学呢', \n",
    "                   '什么呢', '你呢', '什么呢', '怎么样呢', '你呢', '你呢', '你呢', '你呢', '你呢', '中国呢', '节日呢', '你呢', \n",
    "                   '你呢', '你呢', '你呢']\n",
    "\n",
    "L2_ne2_adjusted = ['你呢', '景点呢', '你呢', '你呢', '你呢', '你呢', '你呢', '什么呢', '你呢', '你呢', '你呢', '你呢', '你呢', \n",
    "                   '什么呢', '的呢', '什么呢', '艺术呢', '你呢', '你呢', '你呢', '不同呢', '你呢', '你呢', '你呢', '的呢', \n",
    "                   '你呢', '打算呢', '打算呢', '为什么呢', '了呢', '你呢', '你呢', '你呢', '怎么样呢', '人呢', '你呢', '你呢', \n",
    "                   '什么呢', '家呢', '你呢', '外套呢', '你呢', '习俗呢', '怎么样呢', '你呢', '事情呢', '你呢', '你呢', '的呢', \n",
    "                   '为什么呢', '跟呢', '你呢', '你呢', '你呢', '你呢', '你呢', '部分呢', '方便呢', '过呢', '你呢', '中元节呢', \n",
    "                   '你呢', '然后呢', '你呢', '什么课呢', '研究呢', '好呢', '你呢', '你呢', '你呢', '什么呢']\n",
    "\n",
    "L2_ne3_adjusted =  ['什么呢', '活动呢', '觉得呢', '你呢', '个呢', '你呢', '你呢', '你呢', '你呢', '你呢', '好看呢', '你呢', \n",
    "                   '你呢', '你呢', '你呢', '春假呢', '知道呢', '活动呢', '你呢', '羊杂汤呢', '你呢', '你呢', '你呢', '你呢', \n",
    "                   '你呢', '你呢', '你呢', '人呢', '你呢', '打算呢', '过呢', '打算呢', '专业呢', '你呢', '你呢', '你呢', \n",
    "                   '你呢', '你呢', '你呢', '感恩节呢', '你呢', '你呢', '怎么样呢', '你呢', '你呢', '你们呢', '什么呢', '谁呢', \n",
    "                   '你呢', '你呢', '你呢', '你呢', '什么呢', '你呢', '经历呢', '的呢', '做呢', '你呢', '你呢', '你呢', '你呢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17073170731707318\n",
      "0.3380281690140845\n",
      "0.32786885245901637\n"
     ]
    }
   ],
   "source": [
    "# L2 Type-token ratio of ne in each phase\n",
    "L2_ratio_ne1 = get_ratio(L2_ne1_adjusted)\n",
    "print(L2_ratio_ne1) \n",
    "L2_ratio_ne2 = get_ratio(L2_ne2_adjusted)\n",
    "print(L2_ratio_ne2)\n",
    "L2_ratio_ne3 = get_ratio(L2_ne3_adjusted)\n",
    "print(L2_ratio_ne3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contingency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 NS dataset: Get all the W + SFP constructions (all SFPs all phases in one object)\n",
    "#2 NS dataset: Creat a dictionary with W + SFP as the key and its frequency as the value \n",
    "                # -> for contingency calculation\n",
    "#3 L2 datasets: Get a dictionary with W + SFP as the key and its frequency as the value \n",
    "                # for each SFP in each phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "# All W + SFP constructions from the NS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 127 269\n"
     ]
    }
   ],
   "source": [
    "# get all the a, ba, and ne from the three phases\n",
    "All_NS_a = NS_a1_adjusted + NS_a2_adjusted + NS_a3_adjusted\n",
    "All_NS_ba = NS_ba1_adjusted + NS_ba2_adjusted + NS_ba3_adjusted\n",
    "All_NS_ne = NS_ne1_adjusted + NS_ne2_adjusted + NS_ne3_adjusted\n",
    "\n",
    "# Token frequency of W + a, ba, and ne\n",
    "print(len(All_NS_a), len(All_NS_ba), len(All_NS_ne))\n",
    "\n",
    "# get all the SFPs used by the NS in the control group\n",
    "All_NS_SFPs = All_NS_a + All_NS_ba + All_NS_ne\n",
    "# print(sorted(All_NS_SFPs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2: \n",
    "# Type frequency of W + SFP in the NS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContruction_dic(a_list):\n",
    "    # takes a list as input and returns a dictionary with list elements as the key \n",
    "    # and the elements' frequency as values\n",
    "    result_dic = {}\n",
    "    for i in a_list:\n",
    "        if i in result_dic:\n",
    "            result_dic[i]+=1\n",
    "        else:\n",
    "            result_dic[i]=1\n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct啊:1\n",
      "ent呢:1\n",
      "er吧:1\n",
      "hi吧:1\n",
      "ll啊:1\n"
     ]
    }
   ],
   "source": [
    "# get type frequency of W + SFP\n",
    "All_NS_SFPs_dic = getContruction_dic(All_NS_SFPs)\n",
    "for i in sorted(All_NS_SFPs_dic.keys())[0:5]:\n",
    "    print(\"%s:%s\"% (i, All_NS_SFPs_dic[i])) # formatting: W + SFP: frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: \n",
    "# Frequency of W + SFP in the L2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'是啊': 9, '这样啊': 1, '可怜啊': 1, '对啊': 13, '好看啊': 1, '难啊': 1, '好啊': 7, '的啊': 1, '了啊': 1, '有趣啊': 1, '冰岛啊': 1, '那里啊': 1, '好吃啊': 2, '父母啊': 1, '可爱啊': 1, '烂漫啊': 1}\n",
      "{'没有啊': 3, '够了啊': 1, '可怜啊': 2, '工作啊': 1, '是啊': 13, '对啊': 30, '好吃啊': 4, '好看啊': 1, '确实啊': 1, '人啊': 1, '嗯对啊': 2, 'AC啊': 1, '好啊': 9, '中国啊': 1, '热心啊': 1, '对不起啊': 1, '难啊': 4, '意思啊': 1, '独子啊': 1, '不起啊': 1, '好玩啊': 1, '喜欢啊': 2, '聊天啊': 1, '收到啊': 1, '有啊': 1, '是的啊': 1, '电池啊': 1, '厉害啊': 2, '懂行啊': 1, '糟糕啊': 1, '有趣啊': 1, '熊猫啊': 1, '可惜啊': 1, '有意思啊': 1, '那样啊': 1, '那好啊': 1, '没错啊': 1, '忙啊': 1, '了啊': 1, '听过啊': 1, '养啊': 1, '鱼啊': 2, '不错啊': 1}\n",
      "{'是的啊': 2, '年级啊': 1, '决定啊': 1, '这样子啊': 1, '照片啊': 1, '谢谢啊': 1, '是啊': 17, '你啊': 1, '有意思啊': 4, '对啊': 10, '城市啊': 2, '安排啊': 1, '好玩啊': 2, '没错啊': 1, '好啊': 7, '对不起啊': 1, '好吃啊': 3, '忙啊': 1, '知道啊': 1, '嗯对啊': 1, '北方啊': 1, '火锅啊': 1, '香啊': 1, '试啊': 1, 'ht啊': 1, '这样啊': 1, '万圣节啊': 1, '错啊': 1, '的啊': 1, '厉害啊': 1, '样子啊': 1}\n"
     ]
    }
   ],
   "source": [
    "# frequency of W + a in each phase\n",
    "L2_a1_dic = getContruction_dic(L2_a1_adjusted)\n",
    "print(L2_a1_dic)\n",
    "L2_a2_dic = getContruction_dic(L2_a2_adjusted)\n",
    "print(L2_a2_dic)\n",
    "L2_a3_dic = getContruction_dic(L2_a3_adjusted)\n",
    "print(L2_a3_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of W + ba in each phase\n",
    "L2_ba1_dic = getContruction_dic(L2_ba1_adjusted)\n",
    "# print(L2_ba1_dic)\n",
    "L2_ba2_dic = getContruction_dic(L2_ba2_adjusted)\n",
    "# print(L2_ba2_dic)\n",
    "L2_ba3_dic = getContruction_dic(L2_ba3_adjusted)\n",
    "# print(L2_ba3_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency of W + ne in each phase\n",
    "L2_ne1_dic = getContruction_dic(L2_ne1_adjusted)\n",
    "# L2_ne1_dic\n",
    "L2_ne2_dic = getContruction_dic(L2_ne2_adjusted)\n",
    "# L2_ne2_dic\n",
    "L2_ne3_dic = getContruction_dic(L2_ne3_adjusted)\n",
    "# L2_ne3_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
